{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83d0f3b6-5880-45b1-8d50-ec5fcb41a5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load packages and classes\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tiffslide\n",
    "import seaborn as sns\n",
    "import gget\n",
    "import tifffile\n",
    "import zarr\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# MosaicDataset and BruceDataset classes allow loading and visualisation of the different data sources\n",
    "#from gbmhackathon import MosaicDataset, BruceDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "732d3693-aa30-4b40-b0dd-11a439c8d7de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/gbm_hackathon/notebooks\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2508823e-f4c9-46bb-8474-dbe686080e03",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "scell = np.load('./scFoundation_sc_embedding/embedding.npy')\n",
    "bulk = np.load('./Foundation_bulk/embedding_bulk.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8443cee6-3309-4fbd-8a88-64c052b7229b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18614, 3072)\n",
      "(104, 3072)\n"
     ]
    }
   ],
   "source": [
    "print(scell.shape)\n",
    "print(bulk.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feee7fe3-3768-430e-95ea-c8330309126d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîπ Proje√ß√£o dos embeddings para um espa√ßo comum\n",
    "class ProjectionHead(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(ProjectionHead, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.normalize(self.fc(x), dim=-1)  # Normaliza para facilitar c√°lculo de similaridade\n",
    "\n",
    "# üîπ Defini√ß√£o do modelo\n",
    "class ContrastiveModel(nn.Module):\n",
    "    def __init__(self, dim_img, dim_bulk, dim_mut, output_dim):\n",
    "        super(ContrastiveModel, self).__init__()\n",
    "        self.proj_img = ProjectionHead(dim_img, output_dim)\n",
    "        self.proj_bulk = ProjectionHead(dim_bulk, output_dim)\n",
    "        self.proj_mut = ProjectionHead(dim_mut, output_dim)\n",
    "\n",
    "    def forward(self, img, bulk, mut):\n",
    "        z_img = self.proj_img(img)\n",
    "        z_bulk = self.proj_bulk(bulk)\n",
    "        z_mut = self.proj_mut(mut)\n",
    "        return z_img, z_bulk, z_mut\n",
    "\n",
    "# üîπ Loss Contrastiva (InfoNCE Loss)\n",
    "def contrastive_loss(z1, z2, temperature=0.1):\n",
    "    batch_size = z1.shape[0]\n",
    "    sim_matrix = torch.mm(z1, z2.T)  # Produto interno dos embeddings\n",
    "    sim_matrix /= temperature\n",
    "    labels = torch.arange(batch_size).to(z1.device)  # √çndices como labels\n",
    "    return F.cross_entropy(sim_matrix, labels)\n",
    "\n",
    "# üîπ Dados Fict√≠cios (Substitua pelos seus embeddings reais)\n",
    "N, d1, d2, d3 = 100, 512, 256, 128  # 100 pacientes, dimens√µes diferentes\n",
    "emb_img = torch.randn(N, d1)\n",
    "emb_bulk = torch.randn(N, d2)\n",
    "emb_mut = torch.randn(N, d3)\n",
    "\n",
    "# üîπ Treinamento\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = ContrastiveModel(d1, d2, d3, output_dim=64).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# üîπ Loop de Treinamento\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "    z_img, z_bulk, z_mut = model(emb_img.to(device), emb_bulk.to(device), emb_mut.to(device))\n",
    "    \n",
    "    loss = contrastive_loss(z_img, z_bulk) + contrastive_loss(z_img, z_mut) + contrastive_loss(z_bulk, z_mut)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_gbmhackathon",
   "language": "python",
   "name": "conda_gbmhackathon"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
